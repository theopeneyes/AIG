{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYjm0inkrrUa"
      },
      "source": [
        "Libraries and refrence link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSiXXR-5iCL4",
        "outputId": "baf3c632-f74d-4bd7-a85e-a12b42788443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.10.9-py3-none-any.whl (218 kB)\n",
            "\u001b[K     |████████████████████████████████| 218 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.10.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=d3fa553ff88d0c35bbdde4af2cfe1b13ab7537a40bb356f5ed7ce718825f1b3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=611b5a53b79c1a76c2a6d51d040c2ecda3c94f99f99f32dd77631bb233055b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.9.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.77-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.77\n",
            "  Downloading botocore-1.27.77-py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 25.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.77->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.77->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.24.77 botocore-1.27.77 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n",
            "2022-09-21 13:45:43.066828: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Usage: python -m spacy [OPTIONS] COMMAND [ARGS]...\n",
            "Try 'python -m spacy --help' for help.\n",
            "\n",
            "Error: No such command 'C:Usersdeveloper11DownloadsAI'.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "2022-09-21 13:45:53.682960: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 2.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install docx2txt\n",
        "!pip install -U sentence-transformers\n",
        "!pip install sentence-transformers transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "!python -m spacy C:\\Users\\developer11\\Downloads\\AI Model\\poc3 RH en_core_web_lg en\n",
        "!pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install language_tool_python\n",
        "# https://medium.com/featurepreneur/question-generator-d21265c0648f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r1ELSNfBkGk"
      },
      "source": [
        "## Import Libries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "d1499285fa63463bb72d0fdab3aafecf"
          ]
        },
        "id": "WyVTG1bkmN9u",
        "outputId": "38d8b187-60ca-4612-87d1-1a0e2b8ba263"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1499285fa63463bb72d0fdab3aafecf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import docx2txt\n",
        "import urllib.request\n",
        "import requests, PyPDF2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import time\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertTokenizer, BertModel, AutoTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import spacy\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from warnings import filterwarnings as filt\n",
        "from warnings import filterwarnings as filt\n",
        "filt('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1j-hRY3mrOAU"
      },
      "outputs": [],
      "source": [
        "start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABVWAlL4B1lW"
      },
      "source": [
        "## **Import File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HKc-ytTsvcOs"
      },
      "outputs": [],
      "source": [
        "request=requests.get(\"https://generate-questions.devbyopeneyes.com/api/getFileData/6321af017e09973e7309fea2\")\n",
        "resp= request.json()\n",
        "file_path=(resp[\"data\"][\"file_path\"])\n",
        "_id=(resp[\"data\"][\"_id\"])\n",
        "number_of_question=(resp[\"data\"][\"number_of_question\"])\n",
        "file_type=(resp[\"data\"][\"file_type\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHds-ZlQiCLY"
      },
      "source": [
        "## Read Document : txt, pdf, docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MNuNQIcanxKo"
      },
      "outputs": [],
      "source": [
        "if file_type==\"txt\":\n",
        "  \n",
        "  response = urllib.request.urlopen(file_path)\n",
        "  html = response.read()\n",
        "  text=html.decode('utf8')\n",
        "\n",
        "elif file_type==\"pdf\":\n",
        "  url = file_path\n",
        "  response = requests.get(url)\n",
        "  my_raw_data = response.content\n",
        "\n",
        "  with open(\"my_pdf.pdf\", 'wb') as my_data:\n",
        "      my_data.write(my_raw_data)\n",
        "\n",
        "  open_pdf_file = open(\"my_pdf.pdf\", 'rb')\n",
        "  read_pdf = PyPDF2.PdfFileReader(open_pdf_file)\n",
        "  if read_pdf.isEncrypted:\n",
        "      read_pdf.decrypt(\"\")\n",
        "      text = read_pdf.getPage(0).extractText()\n",
        "\n",
        "  else:\n",
        "      text = read_pdf.getPage(0).extractText()\n",
        "        \n",
        "elif file_type==\"docx\":\n",
        "  url = file_path\n",
        "  response = requests.get(url)\n",
        "  my_raw_data = response.content\n",
        "\n",
        "  with open(\"my_doc.txt\", \"wb\") as text_file:\n",
        "    text_file.write(my_raw_data)\n",
        "\n",
        "  open_docx_file = open(\"my_doc.txt\", 'rb')\n",
        "\n",
        "  text = docx2txt.process(open_docx_file)\n",
        "\n",
        "else:\n",
        "  print(\"Invalid File Type\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHL2AlpNiCLa"
      },
      "source": [
        "## File Validation : Empty file, File can contain special character only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aaXiJjmXP2e2",
        "outputId": "284c755c-2d3d-4b9d-87b2-09db11458b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid file\n"
          ]
        }
      ],
      "source": [
        "if len(text)==0:\n",
        "  print(\"file is empty.\")\n",
        "else:\n",
        "  if not re.match(r'!@#$%^&\"*()-+?_=,<>/\\'', text) :\n",
        "    print(\"Valid file\")\n",
        "  elif text==\"'\":\n",
        "    print(\"invalidd\")\n",
        "  else:\n",
        "    print('Invalid file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcqQLff5QP02"
      },
      "source": [
        "## **Data Cleaning And Data Preprossing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coEXVoTYgS5f"
      },
      "source": [
        "**Split text file into sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5RWwGtM5mc1d"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(text)\n",
        "text_to_sentence= np.array(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CJZ2B-hQgvM"
      },
      "source": [
        "**Split sentences into words by white space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pjd3pJ8dmeqE"
      },
      "outputs": [],
      "source": [
        "sentence_to_words=[]\n",
        "for i in text_to_sentence[0:]:\n",
        "    sentence_to_words.extend(i.split()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRKUAw3AQn29"
      },
      "source": [
        "**Remove Unwanted Words** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yf57Z-wsmh20"
      },
      "outputs": [],
      "source": [
        "tokens = [ w for w in sentence_to_words if w[0]!='[' and w[-1]!= ']' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTuJf9iRUZeY"
      },
      "source": [
        "**Remove punctuation from each word(except full stop(.))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QiWmZ4j1metJ"
      },
      "outputs": [],
      "source": [
        "remove = string.punctuation\n",
        "remove = remove.replace(\".\", \"\")\n",
        "pattern = r\"[{}]\".format(re.escape(remove))\n",
        "table = str.maketrans('', '', pattern)\n",
        "stripped = [w.translate(table) for w in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUkEdheFUicj"
      },
      "source": [
        "**Join Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QpLUTrsTmpPk"
      },
      "outputs": [],
      "source": [
        "words_to_sentense=' '.join(stripped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRCgsNfrUq0J"
      },
      "source": [
        "## **Generate_WH_questions_using_T5_transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "f0498431369644b7af4f983082517e1f",
            "19fabd8cc7924aac94d48ca6b22e8ff8",
            "7ce2300e9f8145fb9be82bbba4885232",
            "0fe965b6d8ae4797a183453947dae050",
            "1343615b9dab4739b0d68d9af6f4ebb4",
            "2650114b540e4684b405d0acd0bac1c6",
            "36107158082b4018a0ec84729eda9400",
            "0ce9e5d513ac4073b7be1848f8043624",
            "083369ce434a46dda655fa6781c4a056",
            "d1e6e81702ca4925b5f713e616f3660b",
            "42ec573510194bf0a59ee76fc6f9c5a9",
            "e385adc12c63480c9e17fa863bce7ee9",
            "db7acb6d17d948958862db664a6364b5",
            "5e0537ded6594b64b6b8d0c29719efed",
            "0189280d7bb24050966391de9fc153f9",
            "785544b694bc40baa768763131a17013"
          ]
        },
        "id": "D5ufE6dnnpda",
        "outputId": "bc45223e-a2a2-4d1d-a8c8-deab18cb575b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0498431369644b7af4f983082517e1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19fabd8cc7924aac94d48ca6b22e8ff8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ce2300e9f8145fb9be82bbba4885232",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fe965b6d8ae4797a183453947dae050",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1343615b9dab4739b0d68d9af6f4ebb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2650114b540e4684b405d0acd0bac1c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36107158082b4018a0ec84729eda9400",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ce9e5d513ac4073b7be1848f8043624",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "083369ce434a46dda655fa6781c4a056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1e6e81702ca4925b5f713e616f3660b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42ec573510194bf0a59ee76fc6f9c5a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e385adc12c63480c9e17fa863bce7ee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db7acb6d17d948958862db664a6364b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e0537ded6594b64b6b8d0c29719efed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0189280d7bb24050966391de9fc153f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "785544b694bc40baa768763131a17013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAnMzNcBiCLk"
      },
      "source": [
        "## Questions generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-qlZYQWbnsWG"
      },
      "outputs": [],
      "source": [
        "def get_question(sentence, answer):\n",
        "\n",
        "  mdl = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "  tknizer = AutoTokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "\n",
        "  text = \"context: {} answer: {}\".format(sentence,answer)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bAmooExGnxjO"
      },
      "outputs": [],
      "source": [
        "def get_embedding(doc):\n",
        "\n",
        "  bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "  \n",
        "  # txt = '[CLS] ' + doc + ' [SEP]'\n",
        "  tokens = bert_tokenizer.tokenize(txt)\n",
        "  token_idx = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
        "  segment_ids = [1] * len(tokens)\n",
        "\n",
        "  torch_token = torch.tensor([token_idx])\n",
        "  torch_segment = torch.tensor([segment_ids])\n",
        "\n",
        "  return bert_model(torch_token, torch_segment)[-1].detach().numpy()\n",
        "\n",
        "def get_pos(context):\n",
        "  doc = nlp(context)\n",
        "  docs = [d.pos_ for d in doc]\n",
        "  return docs, context.split()\n",
        "\n",
        "def get_sent(context):\n",
        "  doc = nlp(context)\n",
        "  return list(doc.sents)\n",
        "\n",
        "def get_vector(doc):\n",
        "  stop_words = \"english\"\n",
        "  n_gram_range = (1,1)\n",
        "  df = CountVectorizer(ngram_range = n_gram_range, stop_words = stop_words).fit([doc])\n",
        "  return df.get_feature_names()\n",
        "\n",
        "\n",
        "def get_key_words(context, module_type = 't'):\n",
        "  keywords = []\n",
        "  top_n = 5\n",
        "  for txt in get_sent(context):\n",
        "    keywd = get_vector(str(txt))\n",
        "    print(f'vectors : {keywd}')\n",
        "    if module_type == 't':\n",
        "      doc_embedding = get_embedding(str(txt))\n",
        "      keywd_embedding = get_embedding(' '.join(keywd))\n",
        "    else:\n",
        "      doc_embedding = model.encode([str(txt)])\n",
        "      keywd_embedding = model.encode(keywd)\n",
        "    \n",
        "    distances = cosine_similarity(doc_embedding, keywd_embedding)\n",
        "    print(distances)\n",
        "    keywords += [(keywd[index], str(txt)) for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "  return keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipkbQ7jTiCLl"
      },
      "source": [
        "## Stored a Questions, Context, Answer, Score And test Duplicate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz5xMjtSn77X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "txt = words_to_sentense\n",
        "questions_arr=[]\n",
        "\n",
        "for answer, context in get_key_words(txt, 'st'):\n",
        "    questions_arr.append({\"question\":get_question(context, answer),\"context\":context,\"answer\":answer, 'rank':None})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BniNMdGif3c0"
      },
      "source": [
        "### Remove Duplications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFJ6e4QESZYM"
      },
      "outputs": [],
      "source": [
        "new_questions = []\n",
        "for i in questions_arr:\n",
        "  if i not in new_questions:\n",
        "    new_questions.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5en4EiVWiCLx"
      },
      "source": [
        "## Get Questiosn from new_questions for the score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh30kAyQiCLx"
      },
      "outputs": [],
      "source": [
        "scoring = []\n",
        "for i in new_questions:\n",
        "    questions_list = i.get(\"question\")\n",
        "    scoring.append(questions_list)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxgnjcX5iCLy"
      },
      "source": [
        "## Eevaluation of Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KFhXFhyPiCLy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "context = words_to_sentense\n",
        "question_score = []\n",
        "for i in scoring:\n",
        "      final_score_list = question_answerer(question=i, context=context)\n",
        "      final_score = final_score_list.get(\"score\")\n",
        "      final_score =\"%.2f\" % round(final_score*100, 2)\n",
        "      question_score.append(final_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPO5LYpGiCLz"
      },
      "source": [
        "## Replace the score value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dyjgKKJWiCLz"
      },
      "outputs": [],
      "source": [
        "for option_index, option in enumerate(new_questions):\n",
        "    option[\"rank\"] = question_score[option_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W59ZHfaniCLz"
      },
      "outputs": [],
      "source": [
        "final_generated_questions = sorted(new_questions, key=lambda i: i['rank'],reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YayFPgcWiCL0"
      },
      "source": [
        "## Question seperation based on user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_4vf72P9iCL0"
      },
      "outputs": [],
      "source": [
        "user_input=int(number_of_question)\n",
        "if user_input > len(final_generated_questions) or user_input == 0: \n",
        "  print(\"please give value between 1 to {}\".format(len(final_generated_questions)))\n",
        "else:\n",
        "    middle_index = user_input\n",
        "    user_required_questions = final_generated_questions[:middle_index]\n",
        "    other_questions = final_generated_questions[middle_index:]\n",
        "\n",
        "    \n",
        "    for question_index, question in enumerate(user_required_questions):\n",
        "      question[\"question_id\"]= question_index +1\n",
        "\n",
        "    for question_index, question in enumerate(other_questions):\n",
        "      question[\"question_id\"]= question_index +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hRgaobY-rUg0"
      },
      "outputs": [],
      "source": [
        "end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rxNjZsNTrU8z"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "dt1 = datetime.fromtimestamp(start)\n",
        "dt2 = datetime.fromtimestamp(end)\n",
        "total_time = dt2 - dt1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZXEpe9uVRGD"
      },
      "source": [
        "## **Final Output , User Validation & Post Generated Questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wasn5mZB1qo3"
      },
      "outputs": [],
      "source": [
        "url=\"https://generate-questions.devbyopeneyes.com/api/GenerateQuestions\" \n",
        "headers = {'Content-Type':'application/json','Accept':'application/json'}\n",
        "post_array = {  \"id\" : _id,\n",
        "                \"questions\" : user_required_questions,\n",
        "                \"other_questions\" : other_questions,\n",
        "                \"upload_process_time\": str(total_time)\n",
        "    }\n",
        "status = requests.post(url,headers=headers,data=json.dumps(post_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FDL3IihsMhRB"
      },
      "outputs": [],
      "source": [
        "print(post_array)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}