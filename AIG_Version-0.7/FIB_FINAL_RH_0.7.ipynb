{"cells":[{"cell_type":"markdown","metadata":{"id":"jhv20N4TepLA"},"source":["# Fill In Blanks Type Questions"]},{"cell_type":"markdown","source":["# **Install Libraries**"],"metadata":{"id":"slMtAMA6UWPc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"POUBDBDm_cq2"},"outputs":[],"source":["# Installing from https://github.com/boudinfl/pke library for Python Keyword extraction\n","# We use a fixed commit as the later changes might break the code. If it was on pip we would have used exact version number.\n","#https://github.com/sudheernaidu53/Machine-learning-Deep-learning-projects\n","\n","!pip install tika\n","!pip install PyPDF2\n","!pip install docx2txt\n","!pip install pdf2docx\n","!python -m spacy info\n","!pip install aspose-words\n","!pip install --quiet flashtext==2.7\n","!pip install datasets evaluate transformers[sentencepiece]\n","!pip install --quiet git+https://github.com/boudinfl/pke.git@5af1f817e0211c33ac3f90e1e86bb5c1283448e8"]},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"U3Ta_EzjUrCM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iglYVckGuO-G"},"outputs":[],"source":["\n","import re\n","import pke\n","import tika\n","import nltk\n","import time\n","import json\n","import PyPDF2\n","import string\n","import random\n","import requests\n","import docx2txt\n","import traceback\n","import itertools\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","\n","import numpy as np\n","from time import sleep\n","from tika import parser\n","from datetime import date\n","from pprint import pprint\n","import aspose.words as aw\n","from itertools import chain\n","from datetime import datetime\n","from pdf2docx import Converter\n","from nltk import sent_tokenize\n","from nltk.corpus import wordnet\n","from multiprocessing import Pool\n","from requests.api import request\n","from transformers import pipeline\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","from multiprocessing import cpu_count\n","from flashtext import KeywordProcessor\n","from nltk.tokenize import sent_tokenize\n","from concurrent.futures import ProcessPoolExecutor\n","from nltk import pos_tag, word_tokenize, sent_tokenize\n","\n","question_answerer = pipeline(\"question-answering\")"]},{"cell_type":"code","source":["start = time.time()"],"metadata":{"id":"l4Ekqjg7CMcr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# File Read"],"metadata":{"id":"EuHgX6OPUxij"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix4g8UhMLX4Q"},"outputs":[],"source":["temp_id = \"new id\"\n","request=requests.get(\"https://generate-questions.devbyopeneyes.com/api/getFileData/63748882333ec0fbfd0e0db2\")\n","resp= request.json()\n","file_path=(resp[\"data\"][\"file_path\"])\n","_id=(resp[\"data\"][\"_id\"])\n","number_of_question=(resp[\"data\"][\"number_of_question\"])\n","file_type=(resp[\"data\"][\"file_type\"])"]},{"cell_type":"code","source":["if file_type == \"txt\":\n","    import urllib.request\n","    response = urllib.request.urlopen(file_path)\n","    html = response.read()\n","    Text=html.decode('utf8')\n","elif file_type==\"pdf\":\n","    url = file_path\n","    response = requests.get(url)\n","    my_raw_data = response.content\n","\n","    with open(\"my_pdf.pdf\", 'wb') as my_data:\n","        my_data.write(my_raw_data)\n","\n","    open_pdf_file = open(\"my_pdf.pdf\", 'rb')\n","    read_pdf = PyPDF2.PdfFileReader(open_pdf_file)\n","    if read_pdf.isEncrypted:\n","        read_pdf.decrypt(\"\")\n","        Text = read_pdf.getPage(0).extractText()\n","    else:\n","        Text = read_pdf.getPage(0).extractText()\n","elif file_type == \"docx\":\n","    url = file_path\n","    response = requests.get(url)\n","    my_raw_data = response.content\n","\n","    with open(\"my_doc.txt\", \"wb\") as text_file:\n","        text_file.write(my_raw_data)\n","\n","    open_docx_file = open(\"my_doc.txt\", 'rb')\n","\n","    Text = docx2txt.process(open_docx_file)\n","else:\n","    print(\"Invalid File Type\")"],"metadata":{"id":"8upHcq23yiRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if Text == None or len(Text) == 0:\n","  print(\"please enter valid file\")\n","else:\n","  print(\"valid file\")"],"metadata":{"id":"pP-q_WkTIvz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pipeline"],"metadata":{"id":"EIZPMnR_Cplh"}},{"cell_type":"code","source":["def Preprocessing(task1):\n","  sentences = sent_tokenize(task1)\n","  sentence_to_words=[]\n","  for i in sentences[0:]:\n","   sentence_to_words.extend(i.split())  \n","  tokens = [ w for w in sentence_to_words if w[0]!='[' and w[-1]!= ']' ]\n","  remove = string.punctuation\n","  remove = remove.replace(\".\", \"\").replace(\"-\", \"\").replace(\",\", \"\")\n","  pattern = r\"[{}]\".format(re.escape(remove))\n","  table = str.maketrans('', '', pattern)\n","  stripped = [w.translate(table) for w in tokens]\n","  words_to_sentense=' '.join(stripped)\n","  return words_to_sentense\n","\n","def tokenize_sentences(words_to_sentense):   \n","    sentence=[]\n","    sentences = sent_tokenize(words_to_sentense)\n","    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n","    filtered_list_short_sentences = [sent for sent in sentences if len(sent)>30 and len(sent)<200]\n","    return filtered_list_short_sentences\n","\n","\n","def get_noun_adj_verb(text):   \n","    out=[]\n","    try:\n","        extractor = pke.unsupervised.MultipartiteRank()\n","        extractor.load_document(input=text,language='en')\n","        #    not contain punctuation marks or stopwords as candidates.\n","        pos = {'ADJ', 'NOUN'}\n","        stoplist = list(string.punctuation)\n","        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n","        stoplist += stopwords.words('english')\n","        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n","        extractor.candidate_selection(pos=pos)\n","        # 4. build the Multipartite graph and rank candidates using random walk,\n","        #    alpha controls the weight adjustment mechanism, see TopicRank for\n","        #    threshold/method parameters.\n","        extractor.candidate_weighting(alpha=1.1,\n","                                      threshold=0.75,\n","                                      method='average')\n","        keyphrases = extractor.get_n_best(n=30)\n","        for val in keyphrases:\n","            out.append(val[0])\n","    except:\n","        out = []\n","        traceback.print_exc()\n","    return out\n","\n","def keyword(flatten_list): \n","    keywords=[]\n","    for i in flatten_list:\n","        noun_verbs_adj = get_noun_adj_verb(i)\n","        keywords.append(noun_verbs_adj)\n","    return keywords\n","\n","def get_sentences_for_keyword(keywords, sentences):    \n","    keyword_processor = KeywordProcessor()\n","    keyword_sentences = {}\n","    for word in keywords:\n","        keyword_sentences[word] = []\n","        keyword_processor.add_keyword(word)\n","    for sentence in sentences:\n","        keywords_found = keyword_processor.extract_keywords(sentence)\n","        for key in keywords_found:\n","            keyword_sentences[key].append(sentence)\n","    for key in keyword_sentences.keys():\n","        values = keyword_sentences[key]\n","        values = sorted(values, key=len, reverse=True)\n","        keyword_sentences[key] = values   \n","    return keyword_sentences\n","\n","def keyword_sentences(keywords,flatten_list):   \n","    keyword_sentence=[]\n","    for i in keywords:\n","        keyword_sentence_mapping_noun_verbs_adj = get_sentences_for_keyword(i, flatten_list)\n","        keyword_sentence.append(keyword_sentence_mapping_noun_verbs_adj)   \n","    return keyword_sentence\n","\n","def get_fill_in_the_blanks(sentence_mapping):   \n","    out={\"title\":\"Fill in the blanks for these sentences with matching words at the top\"}\n","    blank_sentences = []\n","    processed = []\n","    keys=[]\n","    generated_answer=[]\n","    for key in sentence_mapping:\n","        if len(sentence_mapping[key])>0:\n","            sent = sentence_mapping[key][0]\n","            # Compile a regular expression pattern into a regular expression object, which can be used for matching and other methods\n","            insensitive_sent = re.compile(re.escape(key), re.IGNORECASE)\n","            no_of_replacements =  len(re.findall(re.escape(key),sent,re.IGNORECASE))\n","            line = insensitive_sent.sub(' _________ ', sent)\n","            if (sentence_mapping[key][0] not in processed) and no_of_replacements<2:\n","                generated_answer.append({'question':line, 'context':sentence_mapping[key][0],'answer':key})\n","    out[\"sentences\"]=blank_sentences[:]\n","    out[\"keys\"]=keys[:]    \n","    return generated_answer\n","\n","def fill_in_the__blank(keyword_sentence):   \n","    fill_in_the_blank=[]\n","    for i in keyword_sentence:\n","        fill_in_the_blanks = get_fill_in_the_blanks(i)\n","        fill_in_the_blank.append(fill_in_the_blanks)  \n","    return fill_in_the_blank\n","\n","\n","def question_context_list(fill_in_the_blank):  \n","    question=[]\n","    context=[]\n","    for i in fill_in_the_blank:\n","        for j in i:\n","            questions=j[\"question\"]\n","            question.append(questions)\n","            contexts=j[\"context\"]\n","            context.append(contexts) \n","    return question,context\n","\n","def Ranking(question_list, context_list): \n","  rank=[]\n","  for i, j in zip(question_list, context_list):\n","    d = question_answerer(question=i, context=j)\n","    e=d.get(\"score\")\n","    e_updated=1-float(e)\n","    accuracy_score=\"%.2f\" % round(e_updated*100, 2)\n","    rank.append(accuracy_score) \n","  return rank\n","\n","def Fib_dict(rank,fill_in_the_blank):  \n","    for option_index, option in enumerate(fill_in_the_blank):      \n","        for i in option:\n","            i[\"rank\"] = rank[option_index]\n","    fill_in_the_blanks = list(chain.from_iterable(fill_in_the_blank))   \n","    return fill_in_the_blanks\n","\n","\n","def wrong_right_blanks(fill_in_the_blank):\n","  wrong_blank = []\n","  right_blank = []\n","  for i in fill_in_the_blank:\n","      quest = i.get(\"question\")\n","      if quest[0]=='_' or quest[-2]=='_':\n","          wrong_blank.append(i)\n","      else:\n","          right_blank.append(i)\n","  return  wrong_blank,right_blank\n","\n","def Final_requied_blanks(wrong_blank,right_blank):\n","  reduced_wrong_blank = []\n","  for i in wrong_blank:\n","    ranks = (float(i['rank']) - 20.00)\n","    rank_updated=\"%.2f\" % round(ranks, 2)\n","    if float(i['rank'])  <= 20:\n","      reduced_wrong_blank.append({\"question\": i['question'], \"context\": i['context'],\"answer\":i['answer'], \"rank\" : i['rank']})\n","    else:\n","      reduced_wrong_blank.append({\"question\": i['question'], \"context\": i['context'], \"answer\":i['answer'], \"rank\" : rank_updated})\n","  final_required_data = right_blank + reduced_wrong_blank\n","  return final_required_data\n","\n","\n","def Sorting(final_required_data):\n","  final_data = sorted(final_required_data, key=lambda i:float(i['rank']),reverse=True)\n","  return final_data\n","\n","def flatten(list_of_lists):\n","    return list(itertools.chain.from_iterable(list_of_lists))"],"metadata":{"id":"lGqIwGXttwTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Pipeline(text):\n","    a=Preprocessing(Text)                         # preprocessing\n","    b=tokenize_sentences(a)                       # convert text to tokens\n","    c=keyword(b)                                  # fatch keywords\n","    d=keyword_sentences(c,b)                      # keywords to main sentence\n","    e=fill_in_the__blank(d)                       # generate blanks\n","    f=question_context_list(e)                    # create lists of questions and context for find rank\n","    g=Ranking(f[0],f[1])                          # find rank\n","    h=Fib_dict(g,e)                               # crate a dictionary\n","    i=wrong_right_blanks(h)                       # create a link of blank which have at first and last position\n","    j=Final_requied_blanks(i[0],i[1])             # final blanks\n","\n","    return j"],"metadata":{"id":"KQwy7yP7x4P0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs=Pipeline(Text)\n","\n","x=1\n","Final_list=[]  \n","for i in outputs:\n","  Final_new=[]\n","  i['Paragraph_no']=x\n","  Final_new.append(i)\n","  x+=1\n","  Final_list.append(Final_new)\n","\n","h= flatten(Final_list) # Flattening nested list\n","FIB_Questions=Sorting(h) # Our Final FIB questions with sorting"],"metadata":{"id":"zeBIMIDGMgxb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Duplication Remove"],"metadata":{"id":"PFvSngONU5qi"}},{"cell_type":"code","source":["def duplicate_question(final_required_data):\n","    temp_arr = []\n","    questions_are = []\n","    for question in final_required_data:\n","        if question[\"question\"] not in temp_arr:\n","            temp_arr.append(question[\"question\"])\n","            questions_are.append(question)\n","    return questions_are\n","\n","FIB=duplicate_question(FIB_Questions)"],"metadata":{"id":"v9b3gtsgrC4i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Date & Time"],"metadata":{"id":"M8TZB3KGVAVb"}},{"cell_type":"code","source":["end = time.time()\n","\n","dt1 = datetime.fromtimestamp(start)\n","dt2 = datetime.fromtimestamp(end)\n","total_time = dt2 - dt1\n","\n","generated_date = date.today()"],"metadata":{"id":"lEZYsEKpPky9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Post Questions"],"metadata":{"id":"YgCcAvUwVD_j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XtrsjgBH65j"},"outputs":[],"source":["generate_ques=int(number_of_question)\n","if generate_ques > len(FIB) or generate_ques == 0:\n","    print(\"please give value between 1 to {}\".format(len(FIB)))\n","else:\n","    middle_index = generate_ques\n","    user_required_questions = FIB[:middle_index]\n","    other_questions = FIB[middle_index:]\n","        \n","    for question_index, question in enumerate(user_required_questions):\n","      question[\"question_id\"]= question_index +1\n","\n","    for question_index, question in enumerate(other_questions):\n","      question[\"question_id\"]= question_index +1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWMFdvomxajP"},"outputs":[],"source":["url=\"https://generate-questions.devbyopeneyes.com/api/GenerateQuestions\" \n","headers = {'Content-Type':'application/json','Accept':'application/json'}\n","post_array ={\n","    \"id\" : _id,\n","    \"questions\" : user_required_questions,\n","    \"other_questions\" : other_questions,\n","    \"upload_process_time\": str(total_time),\n","    \"generate_date\": str(generated_date),\n","    \"no_of_question_generate\":str(len(FIB))\n","}\n","status = requests.post(url,headers=headers,data=json.dumps(post_array))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}