{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QE3XvqftArw3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.8.2)\n","Requirement already satisfied: torchmetrics\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.10.3)\n","Requirement already satisfied: tensorboard\u003e=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.9.1)\n","Requirement already satisfied: typing-extensions\u003e=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n","Requirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n","Requirement already satisfied: packaging\u003e=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: PyYAML\u003e=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: fsspec[http]\u003e2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.10.0)\n","Requirement already satisfied: tqdm\u003e=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.1)\n","Requirement already satisfied: torch\u003e=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.1+cu113)\n","Requirement already satisfied: lightning-utilities==0.3.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.0)\n","Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from lightning-utilities==0.3.*-\u003epytorch_lightning) (0.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (2.23.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (3.8.3)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (1.3.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (22.1.0)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (1.8.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (4.0.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (1.3.3)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (0.13.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (6.0.2)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=17.0-\u003epytorch_lightning) (3.0.9)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.3.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (0.6.1)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (0.38.3)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (2.14.1)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (0.4.6)\n","Requirement already satisfied: protobuf\u003c3.20,\u003e=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (3.19.6)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (57.4.0)\n","Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.50.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (3.4.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.0.1)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.15.0)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (0.2.8)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (4.9)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (5.2.0)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (4.13.0)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (3.10.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (0.4.8)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (2022.9.24)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]\u003e2021.06.0-\u003epytorch_lightning) (1.24.3)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.9.1-\u003epytorch_lightning) (3.2.2)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire-\u003elightning-utilities==0.3.*-\u003epytorch_lightning) (2.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003etransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.10.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2022.9.24)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","--2022-11-18 07:01:06--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221118%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20221118T070106Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=d0a262fabd32da521c9e4589b706ea8b13ccc75f38a96178979b79f93d8f1f7c\u0026X-Amz-SignedHeaders=host\u0026actor_id=0\u0026key_id=0\u0026repo_id=50261113\u0026response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz\u0026response-content-type=application%2Foctet-stream [following]\n","--2022-11-18 07:01:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221118%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20221118T070106Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=d0a262fabd32da521c9e4589b706ea8b13ccc75f38a96178979b79f93d8f1f7c\u0026X-Amz-SignedHeaders=host\u0026actor_id=0\u0026key_id=0\u0026repo_id=50261113\u0026response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz\u0026response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 600444501 (573M) [application/octet-stream]\n","Saving to: ‘s2v_reddit_2015_md.tar.gz.3’\n","\n","            s2v_red  13%[=\u003e                  ]  80.01M  6.58MB/s    eta 67s    ^C\n","./._s2v_old\n","./s2v_old/\n","./s2v_old/._freqs.json\n","./s2v_old/freqs.json\n","./s2v_old/._vectors\n","./s2v_old/vectors\n","./s2v_old/._cfg\n","./s2v_old/cfg\n","./s2v_old/._strings.json\n","./s2v_old/strings.json\n","./s2v_old/._key2row\n","./s2v_old/key2row\n","cfg  freqs.json  key2row  strings.json\tvectors\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Collecting sentence-transformers\n","  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n","Requirement already satisfied: torch\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n","Requirement already satisfied: huggingface-hub\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.0)\n","Requirement already satisfied: transformers\u003c5.0.0,\u003e=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.24.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.8.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (21.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.9-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.0.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (0.13.2)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.10.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk-\u003esentence-transformers) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk-\u003esentence-transformers) (7.1.2)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2022.9.24)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003esentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003esentence-transformers) (7.1.2)\n","Installing collected packages: sentence-transformers\n","  Attempting uninstall: sentence-transformers\n","    Found existing installation: sentence-transformers 2.2.0\n","    Uninstalling sentence-transformers-2.2.0:\n","      Successfully uninstalled sentence-transformers-2.2.0\n","Successfully installed sentence-transformers-2.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n","Requirement already satisfied: numpy\u003c1.23.0,\u003e=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sense2vec in /usr/local/lib/python3.7/dist-packages (1.0.3)\n","Requirement already satisfied: importlib-metadata\u003e=0.20 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (4.13.0)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (1.21.6)\n","Requirement already satisfied: catalogue\u003e=0.0.4 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (1.0.2)\n","Requirement already satisfied: srsly\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (1.0.6)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (0.10.1)\n","Requirement already satisfied: spacy\u003c3.0.0,\u003e=2.2.3 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (2.3.8)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003e=0.0.4-\u003esense2vec) (3.10.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003e=0.0.4-\u003esense2vec) (4.1.1)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (1.0.9)\n","Requirement already satisfied: thinc\u003c7.5.0,\u003e=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (7.4.6)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (4.64.1)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (2.0.7)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (57.4.0)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (0.7.9)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (1.1.3)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (2022.9.24)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0,\u003e=2.2.3-\u003esense2vec) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etika) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etika) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etika) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etika) (1.24.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: aspose-words in /usr/local/lib/python3.7/dist-packages (22.11.0)\n"]}],"source":["!pip install pytorch_lightning\n","!pip install transformers\n","!pip install --quiet sense2vec==1.0.3\n","!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n","!tar -xvf  s2v_reddit_2015_md.tar.gz\n","!ls s2v_old\n","!pip install --quiet sentence_transformers==2.2.0\n","!pip install -U sentence-transformers\n","!pip install scipy\n","!pip install sense2vec\n","!pip install tika\n","!pip install aspose-words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7E00xYZfa7_q"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (3.4.1) requires spaCy v3.4 and is incompatible with the current spaCy version (2.3.8). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n","  warnings.warn(warn_msg)\n"]}],"source":["import aspose.words as aw\n","from typing import List, Dict\n","import tqdm.notebook as tq\n","import string\n","import itertools\n","from tqdm.notebook import tqdm\n","import json\n","import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sense2vec import Sense2Vec\n","from collections import OrderedDict\n","from platform import dist\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","from typing import List\n","from nltk.tokenize import sent_tokenize\n","from multiprocessing import Pool\n","from multiprocessing import cpu_count\n","from concurrent.futures import ProcessPoolExecutor\n","import toolz\n","from datetime import datetime\n","import requests\n","import random\n","import nltk\n","import textwrap\n","import time\n","from datetime import date\n","from tika import parser\n","import en_core_web_sm\n","import scipy\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5TokenizerFast as T5Tokenizer\n","    )\n","from sentence_transformers import SentenceTransformer\n","model_mcq= SentenceTransformer('all-MiniLM-L12-v2')\n","model_BERT = SentenceTransformer('bert-base-nli-mean-tokens')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w2zuqf3q1BB9"},"outputs":[],"source":["start = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9XWD_J39vqZz"},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}],"source":["question_answerer = pipeline(\"question-answering\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JiyCT7hba8yL"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["MODEL_NAME = 't5-small'\n","SOURCE_MAX_TOKEN_LEN = 64\n","TARGET_MAX_TOKEN_LEN = 24\n","LEARNING_RATE = 0.0001\n","SOURCE_MAX_TOKEN_LEN = 512\n","TARGET_MAX_TOKEN_LEN = 64\n","SEP_TOKEN = '\u003csep\u003e'\n","TOKENIZER_LEN = 32101 #after adding the new \u003csep\u003e token\n","SOURCE_MAX_TOKEN_LEN = 300\n","TARGET_MAX_TOKEN_LEN = 80\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mywaZC-b_ZPP"},"outputs":[],"source":["class QGModel(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n","        self.model.resize_token_embeddings(TOKENIZER_LEN) #resizing after adding new tokens to the tokenizer\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        return output.loss, output.logits\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","        loss, output = self(input_ids, attention_mask, labels)\n","        self.log('train_loss', loss, prog_bar=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","        loss, output = self(input_ids, attention_mask, labels)\n","        self.log('val_loss', loss, prog_bar=True, logger=True)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","        loss, output = self(input_ids, attention_mask, labels)\n","        self.log('test_loss', loss, prog_bar=True, logger=True)\n","        return loss\n","  \n","    def configure_optimizers(self):\n","        return AdamW(self.parameters(), lr=LEARNING_RATE)\n","\n","\n","class AnswerGenerator():\n","    def __init__(self):\n","        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","\n","        checkpoint_path = 'app/ml_models/answer_generation/models/squad-answer-generation.ckpt'\n","        self.ag_model = QGModel.load_from_checkpoint(checkpoint_path)\n","        self.ag_model.freeze()\n","        self.ag_model.eval()\n","\n","    def generate(self, context, generate_count):\n","        model_output = self._model_predict(context, generate_count)\n","\n","        answers = model_output.replace('\u003cpad\u003e', '').split('\u003c/s\u003e')[:-1]\n","\n","        return answers\n","\n","    def _model_predict(self, context: str, generate_count: int) -\u003e str:\n","        source_encoding = self.tokenizer(\n","            context,\n","            max_length=SOURCE_MAX_TOKEN_LEN,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        generated_ids = self.ag_model.model.generate(\n","            input_ids=source_encoding['input_ids'],\n","            attention_mask=source_encoding['attention_mask'],\n","            num_beams=generate_count,\n","            num_return_sequences=generate_count,\n","            max_length=TARGET_MAX_TOKEN_LEN,\n","            repetition_penalty=2.5,\n","            length_penalty=1.0,\n","            early_stopping=True,\n","            use_cache=True\n","        )\n","\n","        preds = {\n","            self.tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n","            for generated_id in generated_ids\n","        }\n","\n","        return ''.join(preds)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y_f6cKduCjU9"},"outputs":[],"source":["class DistractorGenerator():\n","    def __init__(self):\n","        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","        # print('tokenizer len before: ', len(self.tokenizer))\n","        self.tokenizer.add_tokens(SEP_TOKEN)\n","        # print('tokenizer len after: ', len(self.tokenizer))\n","        self.tokenizer_len = len(self.tokenizer)\n","\n","        checkpoint_path = '/content/drive/MyDrive/Leaf-Question-Generation-main/app/ml_models/distractor_generation/models/race-distractors.ckpt'\n","        self.dg_model = QGModel.load_from_checkpoint(checkpoint_path)\n","        self.dg_model.freeze()\n","        self.dg_model.eval()\n","\n","    def generate(self, generate_count, correct, question, context):\n","        \n","        generate_triples_count = int(generate_count / 3) + 1 #since this model generates 3 distractors per generation\n","        \n","        model_output = self._model_predict(generate_triples_count, correct, question, context)\n","\n","        cleaned_result = model_output.replace('\u003cpad\u003e', '').replace('\u003c/s\u003e', '\u003csep\u003e')\n","        cleaned_result = self._replace_all_extra_id(cleaned_result)\n","        distractors = cleaned_result.split('\u003csep\u003e')[:-1]\n","        distractors = [x.translate(str.maketrans('', '', string.punctuation)) for x in distractors]\n","        distractors = list(map(lambda x: x.strip(), distractors))\n","\n","        return distractors\n","\n","    def _model_predict(self, generate_count: int, correct: str, question: str, context: str) -\u003e str:\n","        source_encoding = self.tokenizer(\n","            '{} {} {} {} {}'.format(correct, SEP_TOKEN, question, SEP_TOKEN, context),\n","            max_length= SOURCE_MAX_TOKEN_LEN,\n","            padding='max_length',\n","            truncation= True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","            )\n","\n","        generated_ids = self.dg_model.model.generate(\n","            input_ids=source_encoding['input_ids'],\n","            attention_mask=source_encoding['attention_mask'],\n","            num_beams=generate_count,\n","            num_return_sequences=generate_count,\n","            max_length=TARGET_MAX_TOKEN_LEN,\n","            repetition_penalty=2.5,\n","            length_penalty=1.0,\n","            early_stopping=True,\n","            use_cache=True\n","        )\n","\n","        preds = {\n","            self.tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n","            for generated_id in generated_ids\n","        }\n","\n","        return ''.join(preds)\n","\n","    def _correct_index_of(self, text:str, substring: str, start_index: int = 0):\n","        try:\n","            index = text.index(substring, start_index)\n","        except ValueError:\n","            index = -1\n","\n","        return index\n","\n","    def _replace_all_extra_id(self, text: str):\n","        new_text = text\n","        start_index_of_extra_id = 0\n","\n","        while (self._correct_index_of(new_text, '\u003cextra_id_') \u003e= 0):\n","            start_index_of_extra_id = self._correct_index_of(new_text, '\u003cextra_id_', start_index_of_extra_id)\n","            end_index_of_extra_id = self._correct_index_of(new_text, '\u003e', start_index_of_extra_id)\n","\n","            new_text = new_text[:start_index_of_extra_id] + '\u003csep\u003e' + new_text[end_index_of_extra_id + 1:]\n","\n","        return new_text\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"P9WgBH9pDUHt"},"outputs":[],"source":["class QuestionGenerator():\n","    def __init__(self):\n","        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","        # print('tokenizer len before: ', len(self.tokenizer))\n","        self.tokenizer.add_tokens(SEP_TOKEN)\n","        # print('tokenizer len after: ', len(self.tokenizer))\n","        self.tokenizer_len = len(self.tokenizer)\n","\n","        checkpoint_path = '/content/drive/MyDrive/Leaf-Question-Generation-main/app/ml_models/question_generation/models/multitask-qg-ag.ckpt'\n","        self.qg_model = QGModel.load_from_checkpoint(checkpoint_path)\n","        self.qg_model.freeze()\n","        self.qg_model.eval()\n","\n","    def generate(self, answer: str, context: str) -\u003e str:\n","        model_output = self._model_predict(answer, context)\n","\n","        generated_answer, generated_question = model_output.split('\u003csep\u003e')\n","\n","        return generated_question\n","\n","    def generate_qna(self, context):\n","        answer_mask = '[MASK]'\n","        model_output = self._model_predict(answer_mask, context)\n","\n","        qna_pair = model_output.split('\u003csep\u003e')\n","\n","        if len(qna_pair) \u003c 2:\n","            generated_answer = ''\n","            generated_question = qna_pair[0]\n","        else:\n","            generated_answer = qna_pair[0]\n","            generated_question = qna_pair[1]\n","  \n","        return generated_answer, generated_question, context\n","\n","    def _model_predict(self, answer: str, context: str) -\u003e str:\n","        source_encoding = self.tokenizer(\n","            '{} {} {}'.format(answer, SEP_TOKEN, context),\n","            max_length=SOURCE_MAX_TOKEN_LEN,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        generated_ids = self.qg_model.model.generate(\n","            input_ids=source_encoding['input_ids'],\n","            attention_mask=source_encoding['attention_mask'],\n","            num_beams=16,\n","            max_length=TARGET_MAX_TOKEN_LEN,\n","            repetition_penalty=2.5,\n","            length_penalty=1.0,\n","            early_stopping=True,\n","            use_cache=True\n","        )\n","\n","        preds = {\n","            self.tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for generated_id in generated_ids\n","        }\n","\n","        return ''.join(preds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7gZPCzWrDlXN"},"outputs":[],"source":["class Sense2VecDistractorGeneration():\n","    def __init__(self):\n","        self.s2v = Sense2Vec().from_disk('/content/drive/MyDrive/Leaf-Question-Generation-main/app/ml_models/sense2vec_distractor_generation/data/s2v_old')\n","\n","    def generate(self, answer, desired_count):\n","        distractors = []\n","        answer = answer.lower()\n","        answer = answer.replace(\" \", \"_\")\n","\n","        sense = self.s2v.get_best_sense(answer)\n","\n","        if not sense:\n","            return []\n","\n","        most_similar = self.s2v.most_similar(sense, n=desired_count)\n","\n","        for phrase in most_similar:\n","            normalized_phrase = phrase[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n","\n","            if normalized_phrase.lower() != answer: #TODO: compare the stem of the words (e.g. wrote, writing)\n","                distractors.append(normalized_phrase.capitalize())\n","\n","        return list(OrderedDict.fromkeys(distractors))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ock4RqjYDy_O"},"outputs":[],"source":["class Question:\n","    def __init__(self, answerText:str, questionText: str = '', distractors: List[str] = [], contextText:List[str] = ''):\n","        self.answerText = answerText\n","        self.questionText = questionText\n","        self.distractors = distractors\n","        self.contextText = contextText"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"shFGH0x3EFes"},"outputs":[],"source":["def remove_duplicates(items: List[str]) -\u003e List[str]:\n","    unique_items = []\n","    normalized_unique_items = []\n","\n","    for item in items:\n","\n","\n","        normalized_item = _normalize_item(item)\n","\n","        if normalized_item not in normalized_unique_items:\n","            unique_items.append(item)\n","            normalized_unique_items.append(normalized_item)\n","\n","    return unique_items\n","\n","def duplicate_question(questions_array):\n","    temp_arr = []\n","    questions_are = []\n","    for question in questions_array:\n","        if question[\"question\"] not in temp_arr:\n","            temp_arr.append(question[\"question\"])\n","            questions_are.append(question)\n","    return questions_are\n","\n","def remove_distractors_duplicate_with_correct_answer(correct: str, distractors: List[str]) -\u003e List[str]:\n","    for distractor in distractors:\n","        if _normalize_item(correct) == _normalize_item(distractor):\n","            distractors.remove(distractor)\n","    \n","    return distractors\n","\n","'''\n","Code from: https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\n","'''\n","def _normalize_item(item) -\u003e str:\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(item))))\n","\n","def _calculate_nltk_bleu(references: List[str], hypothesis: str, bleu_n: int = 1):\n","    if hypothesis == '': \n","        return 0, 0, 0, 0 \n","\n","    # Word tokenize\n","    refs_tokenized = list(map(lambda x: word_tokenize(x), references))\n","    hyp_tokenized = word_tokenize(hypothesis)\n","\n","    # Smoothing function to avoid the cases where it resuts 1.0 in the cases when // Corpus/Sentence contains 0 counts of 2-gram overlaps. BLEU scores might be undesirable; use SmoothingFunction() //\n","    chencherry = SmoothingFunction()\n","    bleu = 0\n","\n","    if bleu_n == 1:\n","        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(1, 0, 0, 0), smoothing_function=chencherry.method2)\n","    elif bleu_n == 2:\n","        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method2)\n","    elif bleu_n == 3: \n","        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method2)\n","    elif bleu_n == 4:\n","        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method2)\n","\n","    return bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fGwJrVjIEJ3t"},"outputs":[],"source":["def clean_text(text):\n","    cleaned_text = _remove_brackets(text)\n","    cleaned_text = _remove_square_brackets(cleaned_text)\n","    cleaned_text = _remove_multiple_spaces(cleaned_text)\n","    cleaned_text = _replace_weird_hyphen(cleaned_text)\n","    print(cleaned_text)\n","    return cleaned_text\n","    \n","\n","def _remove_brackets(text: str) -\u003e str:\n","\n","    return re.sub(r'\\((.*?)\\)', lambda L: '', text)\n","\n","\n","def _remove_square_brackets(text: str) -\u003e str:\n","\n","    return re.sub(r'\\[(.*?)\\]', lambda L: '', text)\n","\n","\n","def _remove_multiple_spaces(text: str) -\u003e str:\n","\n","    return re.sub(' +', ' ', text)\n","\n","\n","def _replace_weird_hyphen(text: str) -\u003e str:\n","\n","    return text.replace('–', '-')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3P8uZso-ESRE"},"outputs":[],"source":["class MCQGenerator():\n","    def __init__(self, is_verbose=False):\n","        start_time = time.perf_counter()\n","        print('Loading ML Models...')\n","\n","        self.question_generator = QuestionGenerator()\n","        print('Loaded QuestionGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n","\n","        self.distractor_generator = DistractorGenerator()\n","        print('Loaded DistractorGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n","\n","        self.sense2vec_distractor_generator = Sense2VecDistractorGeneration()\n","        print('Loaded Sense2VecDistractorGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n","\n","    # Main function\n","    def generate_mcq_questions(self, context, desired_count) -\u003e List[Question]:\n","        cleaned_text =  clean_text(context)\n","        questions = self._generate_question_answer_pairs(cleaned_text, desired_count)\n","        questions = self._generate_distractors(cleaned_text, questions)\n","\n","        return questions\n","\n","    def _generate_answers(self, context: str, desired_count: int) -\u003e List[Question]:\n","        # answers = self.answer_generator.generate(context, desired_count)\n","        answers = self._generate_multiple_answers_according_to_desired_count(context, desired_count)\n","\n","        print(answers)\n","        unique_answers = remove_duplicates(answers)\n","\n","        questions = []\n","        for answer in unique_answers:\n","            questions.append(Question(answer))\n","\n","        return questions\n","\n","    def _generate_questions(self, context: str, questions: List[Question]) -\u003e List[Question]:        \n","        for question in questions:\n","            question.questionText = self.question_generator.generate(question.answerText, context)\n","\n","        return questions\n","\n","    def _generate_question_answer_pairs(self, context, desired_count):\n","        context_splits = self._split_context_according_to_desired_count(context, desired_count)\n","  \n","        questions = []\n","\n","        for split in context_splits:\n","            answer, question, context = self.question_generator.generate_qna(split)\n","            questions.append({\"question\" :question,\"answer\":answer.capitalize(),\"context\": context})\n","\n","        # questions = list(toolz.unique(questions, key=lambda x: x.answerText))\n","        \n","        return questions\n","\n","    def _generate_distractors(self, context, questions):\n","        for question in questions:\n","            t5_distractors =  self.distractor_generator.generate(5, question['answer'], question[\"question\"], context)\n","            s2v_distractors = self.sense2vec_distractor_generator.generate(question['answer'], 10)\n","            distractors = t5_distractors + s2v_distractors\n","            distractors = remove_duplicates(distractors)\n","            distractors = remove_distractors_duplicate_with_correct_answer(question['answer'], distractors)\n","            question[\"distractors\"] =distractors\n","        return questions\n","\n","    # Helper functions \n","    def _generate_answer_for_each_sentence(self, context: str) -\u003e List[str]:\n","        sents = sent_tokenize(context)\n","\n","        answers = []\n","        for sent in sents:\n","            answers.append(self.answer_generator.generate(sent, 1)[0])\n","\n","        return answers\n","\n","    #TODO: refactor to create better splits closer to the desired amount\n","    def _split_context_according_to_desired_count(self, context: str, desired_count: int) -\u003e List[str]:\n","        sents = sent_tokenize(context)\n","        sent_ratio = len(sents) / desired_count\n","\n","        context_splits = []\n","\n","        if sent_ratio \u003c 1:\n","            return sents\n","        else:\n","            take_sents_count = int(sent_ratio + 1)\n","\n","            start_sent_index = 0\n","\n","            while start_sent_index \u003c len(sents):\n","                context_split = ' '.join(sents[start_sent_index: start_sent_index + take_sents_count])\n","                context_splits.append(context_split)\n","                start_sent_index += take_sents_count - 1\n","\n","        return context_splits\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uJMLJD2eI2tG"},"outputs":[],"source":["def Sorting(new_questions):\n","  final_data = sorted(new_questions, key=lambda i: float(i['rank']),reverse=True)\n","  return final_data\n","\n","def flatten(list_of_lists):\n","    return list(itertools.chain.from_iterable(list_of_lists))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SbAKXlilE_cv"},"outputs":[],"source":["def questions_wh_with_rank(mcq):\n","    wh_ques=[]\n","    for i in mcq:\n","        del i[\"distractors\"]\n","        final_score_list = question_answerer(question=i['question'], context=i['context'])\n","        final_score = final_score_list.get(\"score\")\n","        final_score =\"%.2f\" % round(final_score*100, 2)\n","        i.update({'rank':final_score})\n","        wh_ques.append(i)\n","    return wh_ques"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZR4ZzcngN5FO"},"outputs":[{"ename":"KeyError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-16-a89296eb5e5f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://generate-questions.devbyopeneyes.com/api/getFileData/63771eb9c7d2e22a3b0e4922\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtype_of_question\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type_of_question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'data'"]}],"source":["request=requests.get(\"https://generate-questions.devbyopeneyes.com/api/getFileData/63771eb9c7d2e22a3b0e4922\")\n","resp= request.json()\n","file_path=(resp[\"data\"][\"file_path\"])\n","type_of_question=(resp[\"data\"][\"type_of_question\"])\n","_id=(resp[\"data\"][\"_id\"])\n","number_of_question=(resp[\"data\"][\"number_of_question\"])\n","file_type=(resp[\"data\"][\"file_type\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JxS21tWewmkQ"},"outputs":[],"source":["if file_type == \"pdf\":\n","  FileName=file_path\n","  doc = aw.Document(FileName)\n","  doc.save(\"File.docx\")\n","  Parse=parser.from_file(\"File.docx\")\n","  data=[]\n","  if not Parse['content'] is None:\n","    for i in (Parse['content'].strip().split('\\n')): \n","      if len(i.split())\u003c5:\n","        pass\n","      else: \n","        data.append(i)\n","    Text=data[1:-1]\n","  else:\n","    Text = None\n","else:\n","  FileName=file_path\n","  Parse=parser.from_file(FileName)\n","  data=[] \n","  if not Parse['content'] is None:\n","    for i in (Parse['content'].strip().split('\\n')): \n","      if len(i.split())\u003c5:\n","        pass\n","      else: \n","        data.append(i)\n","    Text=data\n","  else:\n","    Text = None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ScDpLBgQVKJA"},"outputs":[],"source":["if Text == None or len(Text) == 0:\n","  print(\"please enter valid file\")\n","else:\n","  print(\"valid file\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y_rGuoDAb4vj"},"outputs":[],"source":["MCQ_Generator = MCQGenerator(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mQabuhTUHIxZ"},"outputs":[],"source":["def Pipeline_MCQ(text):\n","    a=MCQ_Generator.generate_mcq_questions(text,100)        ## for generating questions\n","    b=questions_with_mcq_rank(a)                      ## generating distractors \n","    return b[0]                                             # b[1] to get descriptive questions"]},{"cell_type":"markdown","metadata":{"id":"deZ_39gBEHUb"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ibo8GErMJ-zp"},"outputs":[],"source":["def Pipeline_WH(text):\n","    wh_questions = []\n","    questions_are = MCQ_Generator.generate_mcq_questions(text,100) \n","    all_question =  questions_wh_with_rank(questions_are)\n","    return  all_question "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gdVmVBHVBqVJ"},"outputs":[],"source":[" \n","def listToString(s):\n","    str1 = \"\"\n","    for ele in s:\n","        str1 += ele\n","    return str1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EGS31pWcBsfB"},"outputs":[],"source":["text_new=listToString(Text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C9ZF4NL2wyfJ"},"outputs":[],"source":["get_paragraph = []\n","  \n","if type_of_question == \"MCQ\" : \n","\n","    m= Pipeline_MCQ(text_new) # Flattening nested list\n","    generated_questions_with_duplicate=Sorting(m)\n","    final_generated_questions = duplicate_question(generated_questions_with_duplicate)\n","elif type_of_question == \"WH\":\n","\n","    m= Pipeline_WH(text_new) # Flattening nested list\n","    generated_questions_with_duplicate=Sorting(m)\n","    final_generated_questions = duplicate_question(generated_questions_with_duplicate)\n","else:\n","        print(\"Invalid Question type, please enter valid type\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gnFOlLU8WI7q"},"outputs":[],"source":["final_generated_questions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zlKsU-0PkJbV"},"outputs":[],"source":["len(final_generated_questions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4mADNsQqB_45"},"outputs":[],"source":["total_len_question = len(final_generated_questions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Gy9mbqS6SA4W"},"outputs":[],"source":["end = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Gx56wlLUR-wE"},"outputs":[],"source":["dt1 = datetime.fromtimestamp(start)\n","dt2 = datetime.fromtimestamp(end)\n","total_time = dt2 - dt1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EJoW4d4I-Ij7"},"outputs":[],"source":["number_of_question = int(number_of_question)\n","if number_of_question \u003e len(final_generated_questions) or number_of_question == 0:\n","  print(\"please give value between 1 to {}\".format(len(final_generated_questions)))\n","else:\n","    middle_index = number_of_question\n","    user_required_questions = final_generated_questions[:middle_index]\n","    other_questions = final_generated_questions[middle_index:]\n","   \n","    for question_index, question in enumerate(user_required_questions):\n","      question[\"question_id\"]= question_index +1\n","\n","    for question_index, question in enumerate(other_questions):\n","      question[\"question_id\"]= question_index +1\n","\n","    generated_date = date.today()\n","    url=\"https://generate-questions.devbyopeneyes.com/api/GenerateQuestions\" \n","    headers = {'Content-Type':'application/json','Accept':'application/json'}\n","    post_array ={\n","                \"id\" : _id,\n","                \"questions\" : user_required_questions,\n","                \"other_questions\" : other_questions,\n","                \"upload_process_time\": str(total_time),\n","                \"generate_date\": str(generated_date),\n","                \"no_of_question_generate\":str(total_len_question)\n","            }\n","    status = requests.post(url,headers=headers,data=json.dumps(post_array))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WMUFxbCcQpkX"},"outputs":[],"source":["post_array"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}