{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8c_Rgcfb_7a"
      },
      "outputs": [],
      "source": [
        "#https://github.com/sudheernaidu53/Machine-learning-Deep-learning-projects\n",
        "\n",
        "!pip install PyPDF2\n",
        "!pip install docx2txt\n",
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install language_tool_python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbizSMdeTpSt"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VARInkfNEWfq"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pZ7jDIkauhx",
        "outputId": "d4a3cb88-3da1-444f-96f2-64d04cb9f4dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import requests\n",
        "import docx2txt\n",
        "import PyPDF2\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from datetime import datetime\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "from requests.api import request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8nZjh6s2iiU"
      },
      "source": [
        "## **Import File API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCnjdOJK4nGw"
      },
      "outputs": [],
      "source": [
        "temp_id = \"new id\"\n",
        "request=requests.get(\"https://generate-questions.devbyopeneyes.com/api/getFileData/632b087ba62315a24c028622\")\n",
        "resp= request.json()\n",
        "file_path=(resp[\"data\"][\"file_path\"])\n",
        "_id=(resp[\"data\"][\"_id\"])\n",
        "number_of_question=(resp[\"data\"][\"number_of_question\"])\n",
        "file_type=(resp[\"data\"][\"file_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oISvQmkCqdK5"
      },
      "outputs": [],
      "source": [
        "if file_type == \"txt\":\n",
        "    import urllib.request\n",
        "    response = urllib.request.urlopen(file_path)\n",
        "    html = response.read()\n",
        "    text=html.decode('utf8')\n",
        "elif file_type==\"pdf\":\n",
        "    url = file_path\n",
        "    response = requests.get(url)\n",
        "    my_raw_data = response.content\n",
        "\n",
        "    with open(\"my_pdf.pdf\", 'wb') as my_data:\n",
        "        my_data.write(my_raw_data)\n",
        "\n",
        "    open_pdf_file = open(\"my_pdf.pdf\", 'rb')\n",
        "    read_pdf = PyPDF2.PdfFileReader(open_pdf_file)\n",
        "    if read_pdf.isEncrypted:\n",
        "        read_pdf.decrypt(\"\")\n",
        "        text = read_pdf.getPage(0).extractText()\n",
        "    else:\n",
        "        text = read_pdf.getPage(0).extractText()\n",
        "elif file_type == \"docx\":\n",
        "    url = file_path\n",
        "    response = requests.get(url)\n",
        "    my_raw_data = response.content\n",
        "\n",
        "    with open(\"my_doc.txt\", \"wb\") as text_file:\n",
        "        text_file.write(my_raw_data)\n",
        "\n",
        "    open_docx_file = open(\"my_doc.txt\", 'rb')\n",
        "\n",
        "    text = docx2txt.process(open_docx_file)\n",
        "else:\n",
        "    print(\"Invalid File Type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fncwipsGUXLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7789f4-2c69-4db8-f67a-6832331525f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid\n"
          ]
        }
      ],
      "source": [
        "if len(text)==0:\n",
        "    print(\"file is empty.\")\n",
        "else:\n",
        "    if not re.match(r'!@#$%^&\"*()-+?_=,<>/\\'', text) :\n",
        "        print(\"valid\")\n",
        "    elif text==\"'\":\n",
        "        print(\"invalid\")\n",
        "    else:\n",
        "        print('Invalid file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjmJsJXVE3Mo"
      },
      "source": [
        "## **Data Cleaning And Data Preprossing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqd4hyZ5FP9U"
      },
      "source": [
        "**Split Text Into Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYdBGfAwd8N1"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(text)\n",
        "text_to_sentence= np.array(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJeSjuWFWNH"
      },
      "source": [
        "**Split Sentences Into Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOS99Zs7d-gL"
      },
      "outputs": [],
      "source": [
        "sentence_to_words=[]\n",
        "for i in text_to_sentence[0:]:\n",
        "    sentence_to_words.extend(i.split()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxHakhVieA5J"
      },
      "outputs": [],
      "source": [
        "tokens = [ w for w in sentence_to_words if w[0]!='[' and w[-1]!= ']' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdEJwfQWFehA"
      },
      "source": [
        "**Remove Punctuation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNYJ_Yz_eEJK"
      },
      "outputs": [],
      "source": [
        "remove_punctuation = string.punctuation\n",
        "remove_punctuation = remove_punctuation.replace(\".\", \"\")\n",
        "pattern = r\"[{}]\".format(re.escape(remove_punctuation))\n",
        "table = str.maketrans('', '', pattern)\n",
        "stripped = [w.translate(table) for w in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs7lcxpkFi63"
      },
      "source": [
        "**Join Words Into Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plcvPl41eG9a"
      },
      "outputs": [],
      "source": [
        "words_to_sentense=' '.join(stripped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhWq6qcFpXs"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxYj4sXCayur"
      },
      "outputs": [],
      "source": [
        "def clean_and_tag(paragraph):\n",
        "    para = re.sub(r'[^a-zA-Z0-9 .-]','',paragraph)\n",
        "    tags_dict_sentences = defaultdict(list)\n",
        "    for sentence in sent_tokenize(para):\n",
        "        temp_tags = pos_tag(word_tokenize(sentence))\n",
        "        tags_dict = defaultdict(list)\n",
        "        tags_dict_sentences[sentence] = tags_dict\n",
        "        for i in ((temp_tags)):\n",
        "            tags_dict[i[1]].append(i[0])\n",
        "    return tags_dict_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h20DqIFnFxlk"
      },
      "source": [
        "## **Generate Blanks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsBscX8a0gF"
      },
      "outputs": [],
      "source": [
        "def replaceblank(word, sentence):\n",
        "    temp = re.compile(re.escape(word), re.IGNORECASE)\n",
        "    return temp.sub('__________________', sentence),sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaL6hqCoa2s6"
      },
      "outputs": [],
      "source": [
        "def removeWord(sentence, tags_dict):\n",
        "    words = None\n",
        "    if 'NNP' in tags_dict:\n",
        "        words = tags_dict['NNP']\n",
        "    elif 'NN' in tags_dict:\n",
        "        words = tags_dict['NN']\n",
        "    else:\n",
        "        print(\"NN and NNP not found\")\n",
        "        return (None, sentence, None)\n",
        "    if len(words) > 0:\n",
        "        word = random.choice(words)\n",
        "        replaced = replaceblank(word, sentence)\n",
        "        return (word, sentence, replaced)\n",
        "    else:\n",
        "        print(\"words are empty\")\n",
        "        return (None, sentence, None),sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQRT_SMGa4it"
      },
      "outputs": [],
      "source": [
        "def create_blanks(paragraph,num_of_blanks):\n",
        "    global tags_dict_sentences\n",
        "    tags_dict_sentences = clean_and_tag(paragraph)\n",
        "    # these many blanks are possible, as they can't be more than number of sentences\n",
        "    global possib_blanks\n",
        "    possib_blanks = len(list(tags_dict_sentences.keys()))\n",
        "    print(possib_blanks)\n",
        "    if possib_blanks> num_of_blanks:\n",
        "        # randomly shuffle the sentences\n",
        "        rand_sents = (list(tags_dict_sentences.keys()))\n",
        "        random.shuffle(rand_sents)\n",
        "        \n",
        "        #store the blanks in an array\n",
        "        global blanks_arr\n",
        "        blanks_arr = []\n",
        "        # number of prepared blanks and dummy variable to iterate through rand_sents\n",
        "        prepared_blanks = 0\n",
        "        i = 0\n",
        "        while prepared_blanks<num_of_blanks and (i<len(rand_sents)):\n",
        "            curr_sent_tags = tags_dict_sentences[rand_sents[i]]\n",
        "            (word, sentence, replaced) = removeWord(rand_sents[i], tags_dict_sentences[rand_sents[i]])\n",
        "            if replaced is not None:\n",
        "                blanks_arr.append([replaced,word])\n",
        "                i+=1\n",
        "                prepared_blanks+=1\n",
        "            else:\n",
        "                i+=1\n",
        "        if prepared_blanks<num_of_blanks:\n",
        "            print(\"sorry, couldn't form more than {} blanks\".format(prepared_blanks))\n",
        "        return blanks_arr\n",
        "    # the same as above but asking for input\n",
        "    else:\n",
        "        try:\n",
        "            num_of_blanks  = int(input('''number of blanks you want to create are more\n",
        "            than number of sentences, please give a number less than {}.  \n",
        "            if you want to quit, just press any key other than integer and enter\\n'''.format(\n",
        "            possib_blanks)))\n",
        "            if num_of_blanks<possib_blanks:\n",
        "                flag = 0\n",
        "                rand_sents = (list(tags_dict_sentences.keys()))\n",
        "                random.shuffle(rand_sents)\n",
        "                blanks_arr = []\n",
        "                prepared_blanks = 0\n",
        "                i = 0\n",
        "                while prepared_blanks<num_of_blanks and (i<len(rand_sents)):\n",
        "                    curr_sent_tags = tags_dict_sentences[rand_sents[i]]\n",
        "                    (word, sentence, replaced) = removeWord(rand_sents[i], tags_dict_sentences[rand_sents[i]])\n",
        "                    if replaced is not None:\n",
        "                        blanks_arr.append([replaced,word])\n",
        "                        i+=1\n",
        "                        prepared_blanks+=1\n",
        "                    else:\n",
        "                        i+=1\n",
        "                if prepared_blanks<num_of_blanks:\n",
        "                    print(\"sorry, couldn't form more than {} blanks\".format(prepared_blanks))\n",
        "                return blanks_arr\n",
        "        except:\n",
        "            print('quittin :(')\n",
        "            return None\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju8E6VOOF1BZ"
      },
      "source": [
        "**Take Input From Users locally**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSDx_z559_FM"
      },
      "outputs": [],
      "source": [
        "generate_ques=int(number_of_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV7r_FRDF-C-"
      },
      "source": [
        "## **Total Generated Blanks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beLAoQ_zbFLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71053aeb-0f38-4b27-f8af-e748a8d350d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n",
            "NN and NNP not found\n"
          ]
        }
      ],
      "source": [
        "blanks = create_blanks(words_to_sentense,generate_ques)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibfwvAMoGDQZ"
      },
      "source": [
        "## **Generaed Blanks From Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S85zVFFjnJZy",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baffac0c-6023-4d11-94f5-50e8055d0acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n",
            "NN and NNP not found\n",
            "NN and NNP not found\n",
            "NN and NNP not found\n",
            "NN and NNP not found\n",
            "NN and NNP not found\n",
            "sorry, couldn't form more than 67 blanks\n"
          ]
        }
      ],
      "source": [
        "generated_blanks=[]\n",
        "all_blanks = create_blanks(words_to_sentense,(possib_blanks-1))\n",
        "for blank in all_blanks:\n",
        "    generated_blanks.append(blank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQX842jB2ocl"
      },
      "outputs": [],
      "source": [
        "generated_answer = [{'question': l[0][0], 'context':l[0][1],'answer': l[1]} for l in generated_blanks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r35RzOeXGPe6"
      },
      "source": [
        "## **Remove Duplicate Blanks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXyfx3NioAJc"
      },
      "outputs": [],
      "source": [
        "new_questions = []\n",
        "for i in generated_answer:\n",
        "    if i not in new_questions:\n",
        "        new_questions.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-gg88c6AhaA"
      },
      "outputs": [],
      "source": [
        "scoring = []\n",
        "for i in new_questions:\n",
        "  #e=list(i.keys())[2]\n",
        "   e=i.get(\"context\")\n",
        "   scoring.append(e)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI58Ua70Aoe0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92446d31-b667-4b16-ffae-4fe797e5358b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "context = words_to_sentense\n",
        "de=[]\n",
        "for i in scoring:\n",
        "  d = question_answerer(question=i, context=context)\n",
        "  e=d.get(\"score\")\n",
        "  e_updated=1-float(e)\n",
        "  accuracy_score=\"%.2f\" % round(e_updated*100, 6)\n",
        "  de.append(accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH1j3czeAq9i"
      },
      "outputs": [],
      "source": [
        "for option_index, option in enumerate(new_questions):\n",
        "    option[\"rank\"] = float(de[option_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5NmQgmHdaG4"
      },
      "outputs": [],
      "source": [
        "final_generated_questions = sorted(new_questions, key=lambda i: i['rank'], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_blank = []\n",
        "right_blank = []\n",
        "for i in final_generated_questions:\n",
        "    quest = i.get(\"question\")\n",
        "    if quest[0]=='_' or quest[-2]=='_':\n",
        "        wrong_blank.append(i)\n",
        "    else:\n",
        "        right_blank.append(i)"
      ],
      "metadata": {
        "id": "Eg3TXWz_TTY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0_e851rdaG5"
      },
      "source": [
        "## Question seperation based on user input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_wrong_blank = []\n",
        "for i in wrong_blank:\n",
        "  rank = (i['rank'] - 20.00)\n",
        "  if i['rank']  <= 20:\n",
        "    reduced_wrong_blank.append({'question': i['question'], 'answer':i['answer'], 'context': i['context'], 'rank' : i['rank']})\n",
        "  else:\n",
        "    reduced_wrong_blank.append({'question': i['question'], 'answer':i['answer'], 'context': i['context'], 'rank' : rank})"
      ],
      "metadata": {
        "id": "raabkJtjTblU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_required_data = right_blank + reduced_wrong_blank\n",
        "final_data = sorted(final_required_data, key=lambda i: i['rank'],reverse=True)"
      ],
      "metadata": {
        "id": "7JfU6HiAkLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEbjLOm5daG5"
      },
      "outputs": [],
      "source": [
        "if generate_ques > len(new_questions) or generate_ques == 0:\n",
        "    print(\"please give value between 1 to {}\".format(len(new_questions)))\n",
        "else:\n",
        "    middle_index = generate_ques\n",
        "    user_required_questions = final_data[:middle_index]\n",
        "    other_questions = final_data[middle_index:]\n",
        "\n",
        "    \n",
        "    for question_index, question in enumerate(user_required_questions):\n",
        "      question[\"question_id\"]= question_index +1\n",
        "\n",
        "    for question_index, question in enumerate(other_questions):\n",
        "      question[\"question_id\"]= question_index +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMmdy-XbTytT"
      },
      "outputs": [],
      "source": [
        "end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U9iAoJJT3Mr"
      },
      "outputs": [],
      "source": [
        "dt1 = datetime.fromtimestamp(start)\n",
        "dt2 = datetime.fromtimestamp(end)\n",
        "total_time = dt2 - dt1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbbTpG6uGXWj"
      },
      "source": [
        "## **Final Output , User Validation & Post Generated Questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmULjmvJ5Rcs"
      },
      "outputs": [],
      "source": [
        "url=\"https://generate-questions.devbyopeneyes.com/api/GenerateQuestions\" \n",
        "headers = {'Content-Type':'application/json','Accept':'application/json'}\n",
        "post_array ={\n",
        "    \"id\" : _id,\n",
        "    \"questions\" : user_required_questions,\n",
        "    \"other_questions\" : other_questions,\n",
        "    \"upload_process_time\": str(total_time)\n",
        "}\n",
        "status = requests.post(url,headers=headers,data=json.dumps(post_array))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}